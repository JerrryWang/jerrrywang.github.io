<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Scrapy框架 | JerryWang</title><meta name="keywords" content="Scrapy框架"><meta name="author" content="JerryWang"><meta name="copyright" content="JerryWang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一 介绍  Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。   Scrapy 是基于tw"><meta property="og:type" content="article"><meta property="og:title" content="Scrapy框架"><meta property="og:url" content="https://jerrrywang.github.io/2019/12/15/Python/%E7%88%AC%E8%99%AB/6.Scrapy%E6%A1%86%E6%9E%B6/index.html"><meta property="og:site_name" content="JerryWang"><meta property="og:description" content="一 介绍  Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。   Scrapy 是基于tw"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://jerrrywang.github.io/2019/12/09/Python/爬虫/爬虫基本原理/view.jpg"><meta property="article:published_time" content="2019-12-15T07:25:05.000Z"><meta property="article:modified_time" content="2022-12-08T06:55:13.000Z"><meta property="article:author" content="JerryWang"><meta property="article:tag" content="Scrapy框架"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://jerrrywang.github.io/2019/12/09/Python/爬虫/爬虫基本原理/view.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://jerrrywang.github.io/2019/12/15/Python/%E7%88%AC%E8%99%AB/6.Scrapy%E6%A1%86%E6%9E%B6/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://jerrrywang.github.io/api/shubiao/Cyan.ttf" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"NL6QRV8F8M",apiKey:"0b99ef5723aa77799f47331a0e3bc4ff",indexName:"my-blog",hits:{per_page:6},languages:{input_placeholder:"搜索文章",hits_empty:"找不到您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，用时 ${time} 毫秒"}},localSearch:void 0,translate:void 0,noticeOutdate:{limitDay:30,position:"top",messagePrev:"本文上次更新于",messageNext:"天前，文章内容可能已经过时。"},highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!0},runtime:"",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:50,languages:{author:"作者: JerryWang",link:"链接: ",source:"来源: JerryWang",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#1f1f1f",position:"bottom-left"},source:{justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js",css:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css"}},isPhotoFigcaption:!0,islazyload:!1,isAnchor:!0}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Scrapy框架",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2022-12-08 14:55:13"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const a=864e5*o,n={value:t,expiry:(new Date).getTime()+a};localStorage.setItem(e,JSON.stringify(n))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise((t,o)=>{const a=document.createElement("script");a.src=e,a.async=!0,a.onerror=o,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme"),o=(new Date).getHours();void 0===t?o<=6||o>=18?activateDarkMode():activateLightMode():"light"===t?activateLightMode():activateDarkMode();const a=saveToLocal.get("aside-status");void 0!==a&&("hide"===a?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script><script src="/js/welcome.js"></script><script src="/js/sweetalert.js"></script><script src="/api/shubiao/sakuraDrops.js"></script><link rel="stylesheet" href="/css/sweetalert.css"><style>#toggle-sidebar{left:100px}</style><link rel="stylesheet" href="/css/custom.css" media="defer" onload='this.media="all"'><link rel="stylesheet" href="https://jerrrywang.github.io/api/npm/hexo-electric-clock-plus/clock.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">155</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">90</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">31</div></a></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时光轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-coffee"></i> <span>留言板</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-bicycle"></i> <span>相册</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-heartbeat"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-motorcycle"></i> <span>清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i> <span>游戏</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-film"></i> <span>电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i> <span>书籍</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://jerrrywang.github.io/2019/12/09/Python/爬虫/爬虫基本原理/view.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">JerryWang</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>时光轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-coffee"></i> <span>留言板</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-bicycle"></i> <span>相册</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-heartbeat"></i> <span>关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-motorcycle"></i> <span>清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad"></i> <span>游戏</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-film"></i> <span>电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i> <span>书籍</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Scrapy框架</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-12-15T07:25:05.000Z" title="发表于 2019-12-15 15:25:05">2019-12-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-08T06:55:13.000Z" title="更新于 2022-12-08 14:55:13">2022-12-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>40分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="Scrapy框架"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="一-介绍"><a href="#一-介绍" class="headerlink" title="一 介绍"></a>一 介绍</h1><p>Scrapy一个开源和协作的框架，其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的，使用它可以以快速、简单、可扩展的方式从网站中提取所需的数据。但目前Scrapy的用途十分广泛，可用于如数据挖掘、监测和自动化测试等领域，也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。</p><p>Scrapy 是基于twisted框架开发而来，twisted是一个流行的事件驱动的python网络框架。因此Scrapy使用了一种非阻塞（又名异步）的代码来实现并发。整体架构大致如下</p><p><strong>The data flow in Scrapy is controlled by the execution engine, and goes like this:</strong></p><ol><li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> gets the initial Requests to crawl from the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spiders">Spider</a>.</li><li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> schedules the Requests in the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a> and asks for the next Requests to crawl.</li><li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a> returns the next Requests to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a>.</li><li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> sends the Requests to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader">Downloader</a>, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware">Downloader Middlewares</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code>process_request()</code></a>).</li><li>Once the page finishes downloading the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader">Downloader</a> generates a Response (with that page) and sends it to the Engine, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader-middleware">Downloader Middlewares</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code>process_response()</code></a>).</li><li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> receives the Response from the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-downloader">Downloader</a> and sends it to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spiders">Spider</a> for processing, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware">Spider Middleware</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code>process_spider_input()</code></a>).</li><li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spiders">Spider</a> processes the Response and returns scraped items and new Requests (to follow) to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a>, passing through the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-spider-middleware">Spider Middleware</a> (see <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code>process_spider_output()</code></a>).</li><li>The <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-engine">Engine</a> sends processed items to <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-pipelines">Item Pipelines</a>, then send processed Requests to the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a> and asks for possible next Requests to crawl.</li><li>The process repeats (from step 1) until there are no more requests from the <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html#component-scheduler">Scheduler</a>.</li></ol><p><strong>Components：</strong></p><ol><li><p>引擎(EGINE)</p><p>引擎负责控制系统所有组件之间的数据流，并在某些动作发生时触发事件。有关详细信息，请参见上面的数据流部分。</p></li><li><p><strong>调度器(SCHEDULER)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL的优先级队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</p></li><li><p><strong>下载器(DOWLOADER)</strong><br>用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的</p></li><li><p><strong>爬虫(SPIDERS)</strong><br>SPIDERS是开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求</p></li><li><p><strong>项目管道(ITEM PIPLINES)</strong><br>在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作</p></li><li><p>下载器中间件(Downloader Middlewares)</p><p>位于Scrapy引擎和下载器之间，主要用来处理从EGINE传到DOWLOADER的请求request，已经从DOWNLOADER传到EGINE的响应response，你可用该中间件做以下几件事</p><ol><li>process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website);</li><li>change received response before passing it to a spider;</li><li>send a new Request instead of passing received response to a spider;</li><li>pass response to a spider without fetching a web page;</li><li>silently drop some requests.</li></ol></li><li><p><strong>爬虫中间件(Spider Middlewares)</strong><br>位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入（即responses）和输出（即requests）</p></li></ol><p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/architecture.html">官网链接：https://docs.scrapy.org/en/latest/topics/architecture.html</a></p><h1 id="二-安装"><a href="#二-安装" class="headerlink" title="二 安装"></a>二 安装</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Windows平台</span></span><br><span class="line">    <span class="number">1</span>、pip3 install wheel <span class="comment">#安装后，便支持通过wheel文件安装软件，wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs</span></span><br><span class="line">    <span class="number">3</span>、pip3 install lxml</span><br><span class="line">    <span class="number">4</span>、pip3 install pyopenssl</span><br><span class="line">    <span class="number">5</span>、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/</span><br><span class="line">    <span class="number">6</span>、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/<span class="comment">#twisted</span></span><br><span class="line">    <span class="number">7</span>、执行pip3 install 下载目录\Twisted-<span class="number">17.9</span><span class="number">.0</span>-cp36-cp36m-win_amd64.whl</span><br><span class="line">    <span class="number">8</span>、pip3 install scrapy</span><br><span class="line">  </span><br><span class="line"><span class="comment">#Linux平台</span></span><br><span class="line">    <span class="number">1</span>、pip3 install scrapy</span><br></pre></td></tr></table></figure><h1 id="三-命令行工具"><a href="#三-命令行工具" class="headerlink" title="三 命令行工具"></a>三 命令行工具</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1 查看帮助</span></span><br><span class="line">    scrapy -h</span><br><span class="line">    scrapy &lt;command&gt; -h</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 有两种命令：其中Project-only必须切到项目文件夹下才能执行，而Global的命令则不需要</span></span><br><span class="line">    Global commands:</span><br><span class="line">        startproject <span class="comment">#创建项目</span></span><br><span class="line">        genspider    <span class="comment">#创建爬虫程序</span></span><br><span class="line">        settings     <span class="comment">#如果是在项目目录下，则得到的是该项目的配置</span></span><br><span class="line">        runspider    <span class="comment">#运行一个独立的python文件，不必创建项目</span></span><br><span class="line">        shell        <span class="comment">#scrapy shell url地址  在交互式调试，如选择器规则正确与否</span></span><br><span class="line">        fetch        <span class="comment">#独立于程单纯地爬取一个页面，可以拿到请求头</span></span><br><span class="line">        view         <span class="comment">#下载完毕后直接弹出浏览器，以此可以分辨出哪些数据是ajax请求</span></span><br><span class="line">        version      <span class="comment">#scrapy version 查看scrapy的版本，scrapy version -v查看scrapy依赖库的版本</span></span><br><span class="line">    Project-only commands:</span><br><span class="line">        crawl        <span class="comment">#运行爬虫，必须创建项目才行，确保配置文件中ROBOTSTXT_OBEY = False</span></span><br><span class="line">        check        <span class="comment">#检测项目中有无语法错误</span></span><br><span class="line">        <span class="built_in">list</span>         <span class="comment">#列出项目中所包含的爬虫名</span></span><br><span class="line">        edit         <span class="comment">#编辑器，一般不用</span></span><br><span class="line">        parse        <span class="comment">#scrapy parse url地址 --callback 回调函数  #以此可以验证我们的回调函数是否正确</span></span><br><span class="line">        bench        <span class="comment">#scrapy bentch压力测试</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3 官网链接</span></span><br><span class="line">    https://docs.scrapy.org/en/latest/topics/commands.html</span><br></pre></td></tr></table></figure><p>示范用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、执行全局命令：请确保不在某个项目的目录下，排除受该项目配置的影响</span></span><br><span class="line">scrapy startproject MyProject</span><br><span class="line"></span><br><span class="line">cd MyProject</span><br><span class="line">scrapy genspider baidu www.baidu.com</span><br><span class="line"></span><br><span class="line">scrapy settings --get XXX <span class="comment">#如果切换到项目目录下，看到的则是该项目的配置</span></span><br><span class="line"></span><br><span class="line">scrapy runspider baidu.py</span><br><span class="line"></span><br><span class="line">scrapy shell https://www.baidu.com</span><br><span class="line">    response</span><br><span class="line">    response.status</span><br><span class="line">    response.body</span><br><span class="line">    view(response)</span><br><span class="line">    </span><br><span class="line">scrapy view https://www.taobao.com <span class="comment">#如果页面显示内容不全，不全的内容则是ajax请求实现的，以此快速定位问题</span></span><br><span class="line"></span><br><span class="line">scrapy fetch --nolog --headers https://www.taobao.com</span><br><span class="line"></span><br><span class="line">scrapy version <span class="comment">#scrapy的版本</span></span><br><span class="line"></span><br><span class="line">scrapy version -v <span class="comment">#依赖库的版本</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、执行项目命令：切到项目目录下</span></span><br><span class="line">scrapy crawl baidu</span><br><span class="line">scrapy check</span><br><span class="line">scrapy <span class="built_in">list</span></span><br><span class="line">scrapy parse http://quotes.toscrape.com/ --callback parse</span><br><span class="line">scrapy bench</span><br><span class="line">    </span><br></pre></td></tr></table></figure><h1 id="四-项目结构以及爬虫应用简介"><a href="#四-项目结构以及爬虫应用简介" class="headerlink" title="四 项目结构以及爬虫应用简介"></a>四 项目结构以及爬虫应用简介</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">project_name/</span><br><span class="line">   scrapy.cfg</span><br><span class="line">   project_name/</span><br><span class="line">       __init__.py</span><br><span class="line">       items.py</span><br><span class="line">       pipelines.py</span><br><span class="line">       settings.py</span><br><span class="line">       spiders/</span><br><span class="line">           __init__.py</span><br><span class="line">           爬虫<span class="number">1.</span>py</span><br><span class="line">           爬虫<span class="number">2.</span>py</span><br><span class="line">           爬虫<span class="number">3.</span>py</span><br></pre></td></tr></table></figure><p>文件说明：</p><ul><li>scrapy.cfg 项目的主配置信息，用来部署scrapy时使用，爬虫相关的配置信息在settings.py文件中。</li><li>items.py 设置数据存储模板，用于结构化数据，如：Django的Model</li><li>pipelines 数据处理行为，如：一般结构化的数据持久化</li><li>settings.py 配置文件，如：递归的层数、并发数，延迟下载等。<strong>强调:配置文件的选项必须大写否则视为无效**</strong>，正确写法USER_AGENT=’xxxx’**</li><li>spiders 爬虫目录，如：创建文件，编写爬虫规则</li></ul><p><em>注意：一般创建爬虫文件时，以网站域名命名</em></p><p>默认只能在cmd中执行爬虫，如果想在pycharm中执行需要做</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在项目目录下新建：entrypoint.py</span></span><br><span class="line"><span class="keyword">from</span> scrapy.cmdline <span class="keyword">import</span> execute</span><br><span class="line">execute([<span class="string">&#x27;scrapy&#x27;</span>, <span class="string">&#x27;crawl&#x27;</span>, <span class="string">&#x27;xiaohua&#x27;</span>])</span><br></pre></td></tr></table></figure><p>关于windows编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys,os</span><br><span class="line">sys.stdout=io.TextIOWrapper(sys.stdout.buffer,encoding=<span class="string">&#x27;gb18030&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="五-Spiders"><a href="#五-Spiders" class="headerlink" title="五 Spiders"></a>五 Spiders</h1><p><strong>1、介绍</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#1、Spiders是由一系列类（定义了一个网址或一组网址将被爬取）组成，具体包括如何执行爬取任务并且如何从页面中提取结构化的数据。</span><br><span class="line"></span><br><span class="line">#2、换句话说，Spiders是你为了一个特定的网址或一组网址自定义爬取和解析页面行为的地方</span><br></pre></td></tr></table></figure><p><strong>2、Spiders会循环做如下事情</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、生成初始的Requests来爬取第一个URLS，并且标识一个回调函数</span></span><br><span class="line">第一个请求定义在start_requests()方法内默认从start_urls列表中获得url地址来生成Request请求，默认的回调函数是parse方法。回调函数在下载完成返回response时自动触发</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、在回调函数中，解析response并且返回值</span></span><br><span class="line">返回值可以<span class="number">4</span>种：</span><br><span class="line">        包含解析数据的字典</span><br><span class="line">        Item对象</span><br><span class="line">        新的Request对象（新的Requests也需要指定一个回调函数）</span><br><span class="line">        或者是可迭代对象（包含Items或Request）</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、在回调函数中解析页面内容</span></span><br><span class="line">通常使用Scrapy自带的Selectors，但很明显你也可以使用Beutifulsoup，lxml或其他你爱用啥用啥。</span><br><span class="line"></span><br><span class="line"><span class="comment">#4、最后，针对返回的Items对象将会被持久化到数据库</span></span><br><span class="line">通过Item Pipeline组件存到数据库：https://docs.scrapy.org/en/latest/topics/item-pipeline.html<span class="comment">#topics-item-pipeline）</span></span><br><span class="line">或者导出到不同的文件（通过Feed exports：https://docs.scrapy.org/en/latest/topics/feed-exports.html<span class="comment">#topics-feed-exports）</span></span><br></pre></td></tr></table></figure><p><strong>3、Spiders总共提供了五种类：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#1、scrapy.spiders.Spider #scrapy.Spider等同于scrapy.spiders.Spider</span><br><span class="line">#2、scrapy.spiders.CrawlSpider</span><br><span class="line">#3、scrapy.spiders.XMLFeedSpider</span><br><span class="line">#4、scrapy.spiders.CSVFeedSpider</span><br><span class="line">#5、scrapy.spiders.SitemapSpider</span><br></pre></td></tr></table></figure><p><strong>4、导入使用</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> Spider,CrawlSpider,XMLFeedSpider,CSVFeedSpider,SitemapSpider</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AmazonSpider</span>(scrapy.Spider): <span class="comment">#自定义类，继承Spiders提供的基类</span></span><br><span class="line">    name = <span class="string">&#x27;amazon&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.amazon.cn&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.amazon.cn/&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p><strong>5、class scrapy.spiders.Spider</strong></p><p>这是最简单的spider类，任何其他的spider类都需要继承它（包含你自己定义的）。</p><p>该类不提供任何特殊的功能，它仅提供了一个默认的start_requests方法默认从start_urls中读取url地址发送requests请求，并且默认parse作为回调函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AmazonSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;amazon&#x27;</span> </span><br><span class="line">    </span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.amazon.cn&#x27;</span>] </span><br><span class="line">    </span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.amazon.cn/&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    custom_settings = &#123;</span><br><span class="line">        <span class="string">&#x27;BOT_NAME&#x27;</span> : <span class="string">&#x27;allen_Spider_Amazon&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;REQUEST_HEADERS&#x27;</span> : &#123;</span><br><span class="line">          <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>定制scrapy.spider属性与方法详解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、name = &#x27;amazon&#x27; </span></span><br><span class="line">定义爬虫名，scrapy会根据该值定位爬虫程序</span><br><span class="line">所以它必须要有且必须唯一（In Python <span class="number">2</span> this must be ASCII only.）</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、allowed_domains = [&#x27;www.amazon.cn&#x27;] </span></span><br><span class="line">定义允许爬取的域名，如果OffsiteMiddleware启动（默认就启动），</span><br><span class="line">那么不属于该列表的域名及其子域名都不允许爬取</span><br><span class="line">如果爬取的网址为：https://www.example.com/<span class="number">1.</span>html，那就添加<span class="string">&#x27;example.com&#x27;</span>到列表.</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、start_urls = [&#x27;http://www.amazon.cn/&#x27;]</span></span><br><span class="line">如果没有指定url，就从该列表中读取url来生成第一个请求</span><br><span class="line"></span><br><span class="line"><span class="comment">#4、custom_settings</span></span><br><span class="line">值为一个字典，定义一些配置信息，在运行爬虫程序时，这些配置会覆盖项目级别的配置</span><br><span class="line">所以custom_settings必须被定义成一个类属性，由于settings会在类实例化前被加载</span><br><span class="line"></span><br><span class="line"><span class="comment">#5、settings</span></span><br><span class="line">通过self.settings[<span class="string">&#x27;配置项的名字&#x27;</span>]可以访问settings.py中的配置，如果自己定义了custom_settings还是以自己的为准</span><br><span class="line"></span><br><span class="line"><span class="comment">#6、logger</span></span><br><span class="line">日志名默认为spider的名字</span><br><span class="line">self.logger.debug(<span class="string">&#x27;=============&gt;%s&#x27;</span> %self.settings[<span class="string">&#x27;BOT_NAME&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#5、crawler：了解</span></span><br><span class="line">该属性必须被定义到类方法from_crawler中</span><br><span class="line"></span><br><span class="line"><span class="comment">#6、from_crawler(crawler, *args, **kwargs)：了解</span></span><br><span class="line">You probably won’t need to override this directly  because the default implementation acts <span class="keyword">as</span> a proxy to the __init__() method, calling it <span class="keyword">with</span> the given arguments args <span class="keyword">and</span> named arguments kwargs.</span><br><span class="line"></span><br><span class="line"><span class="comment">#7、start_requests()</span></span><br><span class="line">该方法用来发起第一个Requests请求，且必须返回一个可迭代的对象。它在爬虫程序打开时就被Scrapy调用，Scrapy只调用它一次。</span><br><span class="line">默认从start_urls里取出每个url来生成Request(url, dont_filter=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#针对参数dont_filter,请看自定义去重规则</span></span><br><span class="line"></span><br><span class="line">如果你想要改变起始爬取的Requests，你就需要覆盖这个方法，例如你想要起始发送一个POST请求，如下</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;myspider&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> [scrapy.FormRequest(<span class="string">&quot;http://www.example.com/login&quot;</span>,</span><br><span class="line">                                   formdata=&#123;<span class="string">&#x27;user&#x27;</span>: <span class="string">&#x27;john&#x27;</span>, <span class="string">&#x27;pass&#x27;</span>: <span class="string">&#x27;secret&#x27;</span>&#125;,</span><br><span class="line">                                   callback=self.logged_in)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">logged_in</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># here you would extract links to follow and return Requests for</span></span><br><span class="line">        <span class="comment"># each of them, with another callback</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#8、parse(response)</span></span><br><span class="line">这是默认的回调函数，所有的回调函数必须返回an iterable of Request <span class="keyword">and</span>/<span class="keyword">or</span> dicts <span class="keyword">or</span> Item objects.</span><br><span class="line"></span><br><span class="line"><span class="comment">#9、log(message[, level, component])：了解</span></span><br><span class="line">Wrapper that sends a log message through the Spider’s logger, kept <span class="keyword">for</span> backwards compatibility. For more information see Logging <span class="keyword">from</span> Spiders.</span><br><span class="line"></span><br><span class="line"><span class="comment">#10、closed(reason)</span></span><br><span class="line">爬虫程序结束时自动触发</span><br></pre></td></tr></table></figure><p>去重规则：去除重复的url</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">去重规则应该多个爬虫共享的，但凡一个爬虫爬取了，其他都不要爬了，实现方式如下</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法一：</span></span><br><span class="line"><span class="number">1</span>、新增类属性</span><br><span class="line">visited=<span class="built_in">set</span>() <span class="comment">#类属性</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>、回调函数parse方法内：</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">    <span class="keyword">if</span> response.url <span class="keyword">in</span> self.visited:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    .......</span><br><span class="line"></span><br><span class="line">    self.visited.add(response.url) </span><br><span class="line"></span><br><span class="line"><span class="comment">#方法一改进：针对url可能过长，所以我们存放url的hash值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        url=md5(response.request.url)</span><br><span class="line">    <span class="keyword">if</span> url <span class="keyword">in</span> self.visited:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    .......</span><br><span class="line"></span><br><span class="line">    self.visited.add(url) </span><br><span class="line"></span><br><span class="line"><span class="comment">#方法二：Scrapy自带去重功能</span></span><br><span class="line">配置文件：</span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&#x27;scrapy.dupefilter.RFPDupeFilter&#x27;</span> <span class="comment">#默认的去重规则帮我们去重，去重规则在内存中</span></span><br><span class="line">DUPEFILTER_DEBUG = <span class="literal">False</span></span><br><span class="line">JOBDIR = <span class="string">&quot;保存范文记录的日志路径，如：/root/&quot;</span>  <span class="comment"># 最终路径为 /root/requests.seen，去重规则放文件中</span></span><br><span class="line"></span><br><span class="line">scrapy自带去重规则默认为RFPDupeFilter，只需要我们指定</span><br><span class="line">Request(...,dont_filter=<span class="literal">False</span>) ，如果dont_filter=<span class="literal">True</span>则告诉Scrapy这个URL不参与去重。</span><br><span class="line"></span><br><span class="line"><span class="comment">#方法三：</span></span><br><span class="line">我们也可以仿照RFPDupeFilter自定义去重规则，</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy.dupefilter <span class="keyword">import</span> RFPDupeFilter，看源码，仿照BaseDupeFilter</span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤一：在项目目录下自定义去重文件dup.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UrlFilter</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.visited = <span class="built_in">set</span>() <span class="comment">#或者放到数据库</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_settings</span>(<span class="params">cls, settings</span>):</span><br><span class="line">        <span class="keyword">return</span> cls()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">request_seen</span>(<span class="params">self, request</span>):</span><br><span class="line">        <span class="keyword">if</span> request.url <span class="keyword">in</span> self.visited:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        self.visited.add(request.url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open</span>(<span class="params">self</span>):  <span class="comment"># can return deferred</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close</span>(<span class="params">self, reason</span>):  <span class="comment"># can return a deferred</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">log</span>(<span class="params">self, request, spider</span>):  <span class="comment"># log that a request has been filtered</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：配置文件settings.py：</span></span><br><span class="line">DUPEFILTER_CLASS = <span class="string">&#x27;项目名.dup.UrlFilter&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 源码分析：</span></span><br><span class="line"><span class="keyword">from</span> scrapy.core.scheduler <span class="keyword">import</span> Scheduler</span><br><span class="line">见Scheduler下的enqueue_request方法：self.df.request_seen(request)</span><br></pre></td></tr></table></figure><p>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例一：</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;example.com&#x27;</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;http://www.example.com/1.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://www.example.com/2.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://www.example.com/3.html&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        self.logger.info(<span class="string">&#x27;A response from %s just arrived!&#x27;</span>, response.url)</span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line"><span class="comment">#例二：一个回调函数返回多个Requests和Items</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;example.com&#x27;</span>]</span><br><span class="line">    start_urls = [</span><br><span class="line">        <span class="string">&#x27;http://www.example.com/1.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://www.example.com/2.html&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;http://www.example.com/3.html&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> h3 <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//h3&#x27;</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> &#123;<span class="string">&quot;title&quot;</span>: h3&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//a/@href&#x27;</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line"><span class="comment">#例三：在start_requests()内直接指定起始爬取的urls，start_urls就没有用了，</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> MyItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;example.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(<span class="string">&#x27;http://www.example.com/1.html&#x27;</span>, self.parse)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(<span class="string">&#x27;http://www.example.com/2.html&#x27;</span>, self.parse)</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(<span class="string">&#x27;http://www.example.com/3.html&#x27;</span>, self.parse)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">for</span> h3 <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//h3&#x27;</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> MyItem(title=h3)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> response.xpath(<span class="string">&#x27;//a/@href&#x27;</span>).extract():</span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url, callback=self.parse)</span><br></pre></td></tr></table></figure><p>参数传递</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">我们可能需要在命令行为爬虫程序传递参数，比如传递初始的url，像这样</span><br><span class="line"><span class="comment">#命令行执行</span></span><br><span class="line">scrapy crawl myspider -a category=electronics</span><br><span class="line"></span><br><span class="line"><span class="comment">#在__init__方法中可以接收外部传进来的参数</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MySpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;myspider&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, category=<span class="literal">None</span>, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">        self.start_urls = [<span class="string">&#x27;http://www.example.com/categories/%s&#x27;</span> % category]</span><br><span class="line">        <span class="comment">#...</span></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment">#注意接收的参数全都是字符串，如果想要结构化的数据，你需要用类似json.loads的方法</span></span><br></pre></td></tr></table></figure><p><strong>6、<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spiders.html#generic-spiders">其他通用Spiders：https://docs.scrapy.org/en/latest/topics/spiders.html#generic-spiders</a></strong></p><h1 id="六-Selectors"><a href="#六-Selectors" class="headerlink" title="六 Selectors"></a>六 Selectors</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#1 //与/</span><br><span class="line">#2 text</span><br><span class="line">#3、extract与extract_first:从selector对象中解出内容</span><br><span class="line">#4、属性：xpath的属性加前缀@</span><br><span class="line">#4、嵌套查找</span><br><span class="line">#5、设置默认值</span><br><span class="line">#4、按照属性查找</span><br><span class="line">#5、按照属性模糊查找</span><br><span class="line">#6、正则表达式</span><br><span class="line">#7、xpath相对路径</span><br><span class="line">#8、带变量的xpath</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">response.selector.css()</span><br><span class="line">response.selector.xpath()</span><br><span class="line">可简写为</span><br><span class="line">response.css()</span><br><span class="line">response.xpath()</span><br><span class="line"></span><br><span class="line"><span class="comment">#1 //与/</span></span><br><span class="line">response.xpath(<span class="string">&#x27;//body/a/&#x27;</span>)<span class="comment">#</span></span><br><span class="line">response.css(<span class="string">&#x27;div a::text&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//body/a&#x27;</span>) <span class="comment">#开头的//代表从整篇文档中寻找,body之后的/代表body的儿子</span></span><br><span class="line">[]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//body//a&#x27;</span>) <span class="comment">#开头的//代表从整篇文档中寻找,body之后的//代表body的子子孙孙</span></span><br><span class="line">[&lt;Selector xpath=<span class="string">&#x27;//body//a&#x27;</span> data=<span class="string">&#x27;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;&#x27;</span>&gt;, &lt;Selector xpath=<span class="string">&#x27;//body//a&#x27;</span> data=<span class="string">&#x27;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;&#x27;</span>&gt;, &lt;Selector xpath=<span class="string">&#x27;//body//a&#x27;</span> data=<span class="string">&#x27;&lt;a href=&quot;</span></span><br><span class="line"><span class="string">image3.html&quot;&gt;Name: My image 3 &lt;&#x27;</span>&gt;, &lt;Selector xpath=<span class="string">&#x27;//body//a&#x27;</span> data=<span class="string">&#x27;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;&#x27;</span>&gt;, &lt;Selector xpath=<span class="string">&#x27;//body//a&#x27;</span> data=<span class="string">&#x27;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;&#x27;</span>&gt;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#2 text</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//body//a/text()&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">&#x27;body a::text&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、extract与extract_first:从selector对象中解出内容</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//div/a/text()&#x27;</span>).extract()</span><br><span class="line">[<span class="string">&#x27;Name: My image 1 &#x27;</span>, <span class="string">&#x27;Name: My image 2 &#x27;</span>, <span class="string">&#x27;Name: My image 3 &#x27;</span>, <span class="string">&#x27;Name: My image 4 &#x27;</span>, <span class="string">&#x27;Name: My image 5 &#x27;</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">&#x27;div a::text&#x27;</span>).extract()</span><br><span class="line">[<span class="string">&#x27;Name: My image 1 &#x27;</span>, <span class="string">&#x27;Name: My image 2 &#x27;</span>, <span class="string">&#x27;Name: My image 3 &#x27;</span>, <span class="string">&#x27;Name: My image 4 &#x27;</span>, <span class="string">&#x27;Name: My image 5 &#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//div/a/text()&#x27;</span>).extract_first()</span><br><span class="line"><span class="string">&#x27;Name: My image 1 &#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">&#x27;div a::text&#x27;</span>).extract_first()</span><br><span class="line"><span class="string">&#x27;Name: My image 1 &#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、属性：xpath的属性加前缀@</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//div/a/@href&#x27;</span>).extract_first()</span><br><span class="line"><span class="string">&#x27;image1.html&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.css(<span class="string">&#x27;div a::attr(href)&#x27;</span>).extract_first()</span><br><span class="line"><span class="string">&#x27;image1.html&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、嵌套查找</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//div&#x27;</span>).css(<span class="string">&#x27;a&#x27;</span>).xpath(<span class="string">&#x27;@href&#x27;</span>).extract_first()</span><br><span class="line"><span class="string">&#x27;image1.html&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#5、设置默认值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//div[@id=&quot;xxx&quot;]&#x27;</span>).extract_first(default=<span class="string">&quot;not found&quot;</span>)</span><br><span class="line"><span class="string">&#x27;not found&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、按照属性查找</span></span><br><span class="line">response.xpath(<span class="string">&#x27;//div[@id=&quot;images&quot;]/a[@href=&quot;image3.html&quot;]/text()&#x27;</span>).extract()</span><br><span class="line">response.css(<span class="string">&#x27;#images a[@href=&quot;image3.html&quot;]/text()&#x27;</span>).extract()</span><br><span class="line"></span><br><span class="line"><span class="comment">#5、按照属性模糊查找</span></span><br><span class="line">response.xpath(<span class="string">&#x27;//a[contains(@href,&quot;image&quot;)]/@href&#x27;</span>).extract()</span><br><span class="line">response.css(<span class="string">&#x27;a[href*=&quot;image&quot;]::attr(href)&#x27;</span>).extract()</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">&#x27;//a[contains(@href,&quot;image&quot;)]/img/@src&#x27;</span>).extract()</span><br><span class="line">response.css(<span class="string">&#x27;a[href*=&quot;imag&quot;] img::attr(src)&#x27;</span>).extract()</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">&#x27;//*[@href=&quot;image1.html&quot;]&#x27;</span>)</span><br><span class="line">response.css(<span class="string">&#x27;*[href=&quot;image1.html&quot;]&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#6、正则表达式</span></span><br><span class="line">response.xpath(<span class="string">&#x27;//a/text()&#x27;</span>).re(<span class="string">r&#x27;Name: (.*)&#x27;</span>)</span><br><span class="line">response.xpath(<span class="string">&#x27;//a/text()&#x27;</span>).re_first(<span class="string">r&#x27;Name: (.*)&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#7、xpath相对路径</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res=response.xpath(<span class="string">&#x27;//a[contains(@href,&quot;3&quot;)]&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">&#x27;img&#x27;</span>)</span><br><span class="line">[&lt;Selector xpath=<span class="string">&#x27;img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&#x27;</span>&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">&#x27;./img&#x27;</span>)</span><br><span class="line">[&lt;Selector xpath=<span class="string">&#x27;./img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&#x27;</span>&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">&#x27;.//img&#x27;</span>)</span><br><span class="line">[&lt;Selector xpath=<span class="string">&#x27;.//img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&#x27;</span>&gt;]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>res.xpath(<span class="string">&#x27;//img&#x27;</span>) <span class="comment">#这就是从头开始扫描</span></span><br><span class="line">[&lt;Selector xpath=<span class="string">&#x27;//img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&#x27;</span>&gt;, &lt;Selector xpath=<span class="string">&#x27;//img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&#x27;</span>&gt;, &lt;Selector xpath=<span class="string">&#x27;//img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&#x27;</span>&gt;, &lt;Selector xpa</span><br><span class="line">th=<span class="string">&#x27;//img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&#x27;</span>&gt;, &lt;Selector xpath=<span class="string">&#x27;//img&#x27;</span> data=<span class="string">&#x27;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&#x27;</span>&gt;]</span><br><span class="line"></span><br><span class="line"><span class="comment">#8、带变量的xpath</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//div[@id=$xxx]/a/text()&#x27;</span>,xxx=<span class="string">&#x27;images&#x27;</span>).extract_first()</span><br><span class="line"><span class="string">&#x27;Name: My image 1 &#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>response.xpath(<span class="string">&#x27;//div[count(a)=$yyy]/@id&#x27;</span>,yyy=<span class="number">5</span>).extract_first() <span class="comment">#求有5个a标签的div的id</span></span><br><span class="line"><span class="string">&#x27;images&#x27;</span></span><br></pre></td></tr></table></figure><p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/selectors.html">https://docs.scrapy.org/en/latest/topics/selectors.html</a></p><h1 id="七-Items"><a href="#七-Items" class="headerlink" title="七 Items"></a>七 Items</h1><p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/items.html">https://docs.scrapy.org/en/latest/topics/items.html</a></p><h1 id="八-Item-Pipeline"><a href="#八-Item-Pipeline" class="headerlink" title="八 Item Pipeline"></a>八 Item Pipeline</h1><p>自定义pipeline</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一：可以写多个Pipeline类</span></span><br><span class="line"><span class="comment">#1、如果优先级高的Pipeline的process_item返回一个值或者None，会自动传给下一个pipline的process_item,</span></span><br><span class="line"><span class="comment">#2、如果只想让第一个Pipeline执行，那得让第一个pipline的process_item抛出异常raise DropItem()</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、可以用spider.name == &#x27;爬虫名&#x27; 来控制哪些爬虫用哪些pipeline</span></span><br><span class="line"></span><br><span class="line">二：示范</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,v</span>):</span><br><span class="line">        self.value = v</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Scrapy会先通过getattr判断我们是否自定义了from_crawler,有则调它来完</span></span><br><span class="line"><span class="string">        成实例化</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        val = crawler.settings.getint(<span class="string">&#x27;MMMM&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(val)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        爬虫刚启动时执行一次</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;000000&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        爬虫关闭时执行一次</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;111111&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 操作并进行持久化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># return表示会被后续的pipeline继续处理</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 表示将item丢弃，不会被后续pipeline处理</span></span><br><span class="line">        <span class="comment"># raise DropItem()</span></span><br></pre></td></tr></table></figure><p>示范</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、settings.py</span></span><br><span class="line">HOST=<span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">PORT=<span class="number">27017</span></span><br><span class="line">USER=<span class="string">&quot;root&quot;</span></span><br><span class="line">PWD=<span class="string">&quot;123&quot;</span></span><br><span class="line">DB=<span class="string">&quot;amazon&quot;</span></span><br><span class="line">TABLE=<span class="string">&quot;goods&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;Amazon.pipelines.CustomPipeline&#x27;</span>: <span class="number">200</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、pipelines.py</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomPipeline</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,host,port,user,pwd,db,table</span>):</span><br><span class="line">        self.host=host</span><br><span class="line">        self.port=port</span><br><span class="line">        self.user=user</span><br><span class="line">        self.pwd=pwd</span><br><span class="line">        self.db=db</span><br><span class="line">        self.table=table</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Scrapy会先通过getattr判断我们是否自定义了from_crawler,有则调它来完</span></span><br><span class="line"><span class="string">        成实例化</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        HOST = crawler.settings.get(<span class="string">&#x27;HOST&#x27;</span>)</span><br><span class="line">        PORT = crawler.settings.get(<span class="string">&#x27;PORT&#x27;</span>)</span><br><span class="line">        USER = crawler.settings.get(<span class="string">&#x27;USER&#x27;</span>)</span><br><span class="line">        PWD = crawler.settings.get(<span class="string">&#x27;PWD&#x27;</span>)</span><br><span class="line">        DB = crawler.settings.get(<span class="string">&#x27;DB&#x27;</span>)</span><br><span class="line">        TABLE = crawler.settings.get(<span class="string">&#x27;TABLE&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> cls(HOST,PORT,USER,PWD,DB,TABLE)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        爬虫刚启动时执行一次</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.client = MongoClient(<span class="string">&#x27;mongodb://%s:%s@%s:%s&#x27;</span> %(self.user,self.pwd,self.host,self.port))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self,spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        爬虫关闭时执行一次</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 操作并进行持久化</span></span><br><span class="line"></span><br><span class="line">        self.client[self.db][self.table].save(<span class="built_in">dict</span>(item))</span><br></pre></td></tr></table></figure><p><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html">https://docs.scrapy.org/en/latest/topics/item-pipeline.html</a></p><h1 id="九-Dowloader-Middeware"><a href="#九-Dowloader-Middeware" class="headerlink" title="九 Dowloader Middeware"></a>九 Dowloader Middeware</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">下载中间件的用途</span><br><span class="line">    <span class="number">1</span>、在process——request内，自定义下载，不用scrapy的下载</span><br><span class="line">    <span class="number">2</span>、对请求进行二次加工，比如</span><br><span class="line">        设置请求头</span><br><span class="line">        设置cookie</span><br><span class="line">        添加代理</span><br><span class="line">            scrapy自带的代理组件：</span><br><span class="line">                <span class="keyword">from</span> scrapy.downloadermiddlewares.httpproxy <span class="keyword">import</span> HttpProxyMiddleware</span><br><span class="line">                <span class="keyword">from</span> urllib.request <span class="keyword">import</span> getproxies</span><br></pre></td></tr></table></figure><p>下载器中间件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DownMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        请求需要被下载时，经过所有下载器中间件的process_request调用</span></span><br><span class="line"><span class="string">        :param request: </span></span><br><span class="line"><span class="string">        :param spider: </span></span><br><span class="line"><span class="string">        :return:  </span></span><br><span class="line"><span class="string">            None,继续后续中间件去下载；</span></span><br><span class="line"><span class="string">            Response对象，停止process_request的执行，开始执行process_response</span></span><br><span class="line"><span class="string">            Request对象，停止中间件的执行，将Request重新调度器</span></span><br><span class="line"><span class="string">            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param result:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">            Response 对象：转交给其他中间件process_response</span></span><br><span class="line"><span class="string">            Request 对象：停止中间件，request会被重新调度下载</span></span><br><span class="line"><span class="string">            raise IgnoreRequest 异常：调用Request.errback</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;response1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param exception:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">            None：继续交给后续中间件处理异常；</span></span><br><span class="line"><span class="string">            Response对象：停止后续process_exception方法</span></span><br><span class="line"><span class="string">            Request对象：停止中间件，request将会被重新调用下载</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>配置代理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、与middlewares.py同级目录下新建proxy_handle.py</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_proxy</span>():</span><br><span class="line">    <span class="keyword">return</span> requests.get(<span class="string">&quot;http://127.0.0.1:5010/get/&quot;</span>).text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">delete_proxy</span>(<span class="params">proxy</span>):</span><br><span class="line">    requests.get(<span class="string">&quot;http://127.0.0.1:5010/delete/?proxy=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(proxy))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="comment">#2、middlewares.py</span></span><br><span class="line"><span class="keyword">from</span> Amazon.proxy_handle <span class="keyword">import</span> get_proxy,delete_proxy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DownMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        请求需要被下载时，经过所有下载器中间件的process_request调用</span></span><br><span class="line"><span class="string">        :param request:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">            None,继续后续中间件去下载；</span></span><br><span class="line"><span class="string">            Response对象，停止process_request的执行，开始执行process_response</span></span><br><span class="line"><span class="string">            Request对象，停止中间件的执行，将Request重新调度器</span></span><br><span class="line"><span class="string">            raise IgnoreRequest异常，停止process_request的执行，开始执行process_exception</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        proxy=<span class="string">&quot;http://&quot;</span> + get_proxy()</span><br><span class="line">        request.meta[<span class="string">&#x27;download_timeout&#x27;</span>]=<span class="number">20</span></span><br><span class="line">        request.meta[<span class="string">&quot;proxy&quot;</span>] = proxy</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;为%s 添加代理%s &#x27;</span> % (request.url, proxy),end=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;元数据为&#x27;</span>,request.meta)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        spider处理完成，返回时调用</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param result:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">            Response 对象：转交给其他中间件process_response</span></span><br><span class="line"><span class="string">            Request 对象：停止中间件，request会被重新调度下载</span></span><br><span class="line"><span class="string">            raise IgnoreRequest 异常：调用Request.errback</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;返回状态吗&#x27;</span>,response.status)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        当下载处理器(download handler)或 process_request() (下载中间件)抛出异常</span></span><br><span class="line"><span class="string">        :param response:</span></span><br><span class="line"><span class="string">        :param exception:</span></span><br><span class="line"><span class="string">        :param spider:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">            None：继续交给后续中间件处理异常；</span></span><br><span class="line"><span class="string">            Response对象：停止后续process_exception方法</span></span><br><span class="line"><span class="string">            Request对象：停止中间件，request将会被重新调用下载</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;代理%s，访问%s出现异常:%s&#x27;</span> %(request.meta[<span class="string">&#x27;proxy&#x27;</span>],request.url,exception))</span><br><span class="line">        <span class="keyword">import</span> time</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        delete_proxy(request.meta[<span class="string">&#x27;proxy&#x27;</span>].split(<span class="string">&quot;//&quot;</span>)[-<span class="number">1</span>])</span><br><span class="line">        request.meta[<span class="string">&#x27;proxy&#x27;</span>]=<span class="string">&#x27;http://&#x27;</span>+get_proxy()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> request</span><br></pre></td></tr></table></figure><h1 id="十-Spider-Middleware"><a href="#十-Spider-Middleware" class="headerlink" title="十 Spider Middleware"></a>十 Spider Middleware</h1><p><strong>1、爬虫中间件方法介绍</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="comment"># Not all methods need to be defined. If a method is not defined,</span></span><br><span class="line">    <span class="comment"># scrapy acts as if the spider middleware does not modify the</span></span><br><span class="line">    <span class="comment"># passed objects.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="comment"># This method is used by Scrapy to create your spiders.</span></span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) <span class="comment">#当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="comment"># spider.logger.info(&#x27;我是allen派来的爬虫1: %s&#x27; % spider.name)</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是allen派来的爬虫1: %s&#x27;</span> % spider.name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">        <span class="comment"># Called with the start requests of the spider, and works</span></span><br><span class="line">        <span class="comment"># similarly to the process_spider_output() method, except</span></span><br><span class="line">        <span class="comment"># that it doesn’t have a response associated.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;start_requests1&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="comment"># Called for each response that goes through the spider</span></span><br><span class="line">        <span class="comment"># middleware and into the spider.</span></span><br><span class="line">        <span class="comment"># 每个response经过爬虫中间件进入spider时调用</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 返回值：Should return None or raise an exception.</span></span><br><span class="line">        <span class="comment">#1、None: 继续执行其他中间件的process_spider_input</span></span><br><span class="line">        <span class="comment">#2、抛出异常：</span></span><br><span class="line">        <span class="comment"># 一旦抛出异常则不再执行其他中间件的process_spider_input</span></span><br><span class="line">        <span class="comment"># 并且触发request绑定的errback</span></span><br><span class="line">        <span class="comment"># errback的返回值倒着传给中间件的process_spider_output</span></span><br><span class="line">        <span class="comment"># 如果未找到errback，则倒着执行中间件的process_spider_exception</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;input1&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="comment"># Called with the results returned from the Spider, after</span></span><br><span class="line">        <span class="comment"># it has processed the response.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Must return an iterable of Request, dict or Item objects.</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;output1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 用yield返回多次，与return返回一次是一个道理</span></span><br><span class="line">        <span class="comment"># 如果生成器掌握不好（函数内有yield执行函数得到的是生成器而并不会立刻执行），生成器的形式会容易误导你对中间件执行顺序的理解</span></span><br><span class="line">        <span class="comment"># for i in result:</span></span><br><span class="line">        <span class="comment">#     yield i</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="comment"># Called when a spider or process_spider_input() method</span></span><br><span class="line">        <span class="comment"># (from other spider middleware) raises an exception.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Should return either None or an iterable of Response, dict</span></span><br><span class="line">        <span class="comment"># or Item objects.</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;exception1&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>2、当前爬虫启动时以及初始请求产生时</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">打开注释：</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware1&#x27;: 200,</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware2&#x27;: 300,</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware3&#x27;: 400,</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：middlewares.py</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened) <span class="comment">#当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是allen派来的爬虫1: %s&#x27;</span> % spider.name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">        <span class="comment"># Must return only requests (not items).</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;start_requests1&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)  <span class="comment"># 当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是allen派来的爬虫2: %s&#x27;</span> % spider.name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;start_requests2&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware3</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        s = cls()</span><br><span class="line">        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)  <span class="comment"># 当前爬虫执行时触发spider_opened</span></span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;我是allen派来的爬虫3: %s&#x27;</span> % spider.name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_start_requests</span>(<span class="params">self, start_requests, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;start_requests3&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> start_requests:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤三：分析运行结果</span></span><br><span class="line"><span class="comment">#1、启动爬虫时则立刻执行：</span></span><br><span class="line"></span><br><span class="line">我是allen派来的爬虫<span class="number">1</span>: baidu</span><br><span class="line">我是allen派来的爬虫<span class="number">2</span>: baidu</span><br><span class="line">我是allen派来的爬虫<span class="number">3</span>: baidu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、然后产生一个初始的request请求，依次经过爬虫中间件1,2,3：</span></span><br><span class="line">start_requests1</span><br><span class="line">start_requests2</span><br><span class="line">start_requests3</span><br></pre></td></tr></table></figure><p><strong>3、process_spider_input返回None时</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：打开注释：</span></span><br><span class="line">SPIDER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="string">&#x27;Baidu.middlewares.SpiderMiddleware1&#x27;</span>: <span class="number">200</span>,</span><br><span class="line">   <span class="string">&#x27;Baidu.middlewares.SpiderMiddleware2&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">&#x27;Baidu.middlewares.SpiderMiddleware3&#x27;</span>: <span class="number">400</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#步骤二：middlewares.py</span></span><br><span class="line"><span class="string">from scrapy import signals</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class SpiderMiddleware1(object):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_input(self, response, spider):</span></span><br><span class="line"><span class="string">        print(&quot;input1&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_output(self, response, result, spider):</span></span><br><span class="line"><span class="string">        print(&#x27;output1&#x27;)</span></span><br><span class="line"><span class="string">        return result</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_exception(self, response, exception, spider):</span></span><br><span class="line"><span class="string">        print(&#x27;exception1&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class SpiderMiddleware2(object):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_input(self, response, spider):</span></span><br><span class="line"><span class="string">        print(&quot;input2&quot;)</span></span><br><span class="line"><span class="string">        return None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_output(self, response, result, spider):</span></span><br><span class="line"><span class="string">        print(&#x27;output2&#x27;)</span></span><br><span class="line"><span class="string">        return result</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_exception(self, response, exception, spider):</span></span><br><span class="line"><span class="string">        print(&#x27;exception2&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class SpiderMiddleware3(object):</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_input(self, response, spider):</span></span><br><span class="line"><span class="string">        print(&quot;input3&quot;)</span></span><br><span class="line"><span class="string">        return None</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_output(self, response, result, spider):</span></span><br><span class="line"><span class="string">        print(&#x27;output3&#x27;)</span></span><br><span class="line"><span class="string">        return result</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def process_spider_exception(self, response, exception, spider):</span></span><br><span class="line"><span class="string">        print(&#x27;exception3&#x27;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#步骤三：运行结果分析</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#1、返回response时，依次经过爬虫中间件1,2,3</span></span><br><span class="line"><span class="string">input1</span></span><br><span class="line"><span class="string">input2</span></span><br><span class="line"><span class="string">input3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#2、spider处理完毕后，依次经过爬虫中间件3,2,1</span></span><br><span class="line"><span class="string">output3</span></span><br><span class="line"><span class="string">output2</span></span><br><span class="line"><span class="string">output1</span></span><br></pre></td></tr></table></figure><p><strong>4、process_spider_input抛出异常时</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">打开注释：</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware1&#x27;: 200,</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware2&#x27;: 300,</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware3&#x27;: 400,</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：middlewares.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;input1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;output1&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;exception1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;input2&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> <span class="type">Type</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;output2&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;exception2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware3</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;input3&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;output3&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;exception3&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="comment">#运行结果        </span></span><br><span class="line">input1</span><br><span class="line">input2</span><br><span class="line">exception3</span><br><span class="line">exception2</span><br><span class="line">exception1</span><br><span class="line"></span><br><span class="line"><span class="comment">#分析：</span></span><br><span class="line"><span class="comment">#1、当response经过中间件1的 process_spider_input返回None，继续交给中间件2的process_spider_input</span></span><br><span class="line"><span class="comment">#2、中间件2的process_spider_input抛出异常，则直接跳过后续的process_spider_input，将异常信息传递给Spiders里该请求的errback</span></span><br><span class="line"><span class="comment">#3、没有找到errback，则该response既没有被Spiders正常的callback执行，也没有被errback执行，即Spiders啥事也没有干，那么开始倒着执行process_spider_exception</span></span><br><span class="line"><span class="comment">#4、如果process_spider_exception返回None，代表该方法推卸掉责任，并没处理异常，而是直接交给下一个process_spider_exception，全都返回None，则异常最终交给Engine抛出</span></span><br></pre></td></tr></table></figure><p><strong>5、指定errback</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#步骤一：spider.py</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BaiduSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;baidu&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;www.baidu.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://www.baidu.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">yield</span> scrapy.Request(url=<span class="string">&#x27;http://www.baidu.com/&#x27;</span>,</span><br><span class="line">                             callback=self.parse,</span><br><span class="line">                             errback=self.parse_err,</span><br><span class="line">                             )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse_err</span>(<span class="params">self,res</span>):</span><br><span class="line">        <span class="comment">#res 为异常信息，异常已经被该函数处理了，因此不会再抛给因此，于是开始走process_spider_output</span></span><br><span class="line">        <span class="keyword">return</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>] <span class="comment">#提取异常信息中有用的数据以可迭代对象的形式存放于管道中，等待被process_spider_output取走</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤二：</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">打开注释：</span></span><br><span class="line"><span class="string">SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware1&#x27;: 200,</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware2&#x27;: 300,</span></span><br><span class="line"><span class="string">   &#x27;Baidu.middlewares.SpiderMiddleware3&#x27;: 400,</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤三：middlewares.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware1</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;input1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;output1&#x27;</span>,<span class="built_in">list</span>(result))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;exception1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware2</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;input2&quot;</span>)</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">&#x27;input2 抛出异常&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;output2&#x27;</span>,<span class="built_in">list</span>(result))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;exception2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SpiderMiddleware3</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_input</span>(<span class="params">self, response, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;input3&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_output</span>(<span class="params">self, response, result, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;output3&#x27;</span>,<span class="built_in">list</span>(result))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_spider_exception</span>(<span class="params">self, response, exception, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;exception3&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#步骤四：运行结果分析</span></span><br><span class="line">input1</span><br><span class="line">input2</span><br><span class="line">output3 [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>] <span class="comment">#parse_err的返回值放入管道中，只能被取走一次，在output3的方法内可以根据异常信息封装一个新的request请求</span></span><br><span class="line">output2 []</span><br><span class="line">output1 []</span><br></pre></td></tr></table></figure><h1 id="十一-自定义扩展"><a href="#十一-自定义扩展" class="headerlink" title="十一 自定义扩展"></a>十一 自定义扩展</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">自定义扩展（与django的信号类似）</span><br><span class="line">    1、django的信号是django是预留的扩展，信号一旦被触发，相应的功能就会执行</span><br><span class="line">    2、scrapy自定义扩展的好处是可以在任意我们想要的位置添加功能，而其他组件中提供的功能只能在规定的位置执行</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1、在与settings同级目录下新建一个文件，文件名可以为extentions.py,内容如下</span></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyExtension</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, value</span>):</span><br><span class="line">        self.value = value</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        val = crawler.settings.getint(<span class="string">&#x27;MMMM&#x27;</span>)</span><br><span class="line">        obj = cls(val)</span><br><span class="line"></span><br><span class="line">        crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)</span><br><span class="line">        crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> obj</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_opened</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;=============&gt;open&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">spider_closed</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;=============&gt;close&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2、配置生效</span></span><br><span class="line">EXTENSIONS = &#123;</span><br><span class="line">    <span class="string">&quot;Amazon.extentions.MyExtension&quot;</span>:<span class="number">200</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="十二-settings-py"><a href="#十二-settings-py" class="headerlink" title="十二 settings.py"></a>十二 settings.py</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==&gt;第一部分：基本配置&lt;===</span></span><br><span class="line"><span class="comment">#1、项目名称，默认的USER_AGENT由它来构成，也作为日志记录的日志名</span></span><br><span class="line">BOT_NAME = <span class="string">&#x27;Amazon&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、爬虫应用路径</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;Amazon.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;Amazon.spiders&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、客户端User-Agent请求头</span></span><br><span class="line"><span class="comment">#USER_AGENT = &#x27;Amazon (+http://www.yourdomain.com)&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、是否遵循爬虫协议</span></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#5、是否支持cookie，cookiejar进行操作cookie，默认开启</span></span><br><span class="line"><span class="comment">#COOKIES_ENABLED = False</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#6、Telnet用于查看当前爬虫的信息，操作爬虫等...使用telnet ip port ，然后通过命令操作</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_ENABLED = False</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_HOST = &#x27;127.0.0.1&#x27;</span></span><br><span class="line"><span class="comment">#TELNETCONSOLE_PORT = [6023,]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#7、Scrapy发送HTTP请求默认使用的请求头</span></span><br><span class="line"><span class="comment">#DEFAULT_REQUEST_HEADERS = &#123;</span></span><br><span class="line"><span class="comment">#   &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,</span></span><br><span class="line"><span class="comment">#   &#x27;Accept-Language&#x27;: &#x27;en&#x27;,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第二部分：并发与延迟&lt;===</span></span><br><span class="line"><span class="comment">#1、下载器总共最大处理的并发请求数,默认值16</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS = 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、每个域名能够被执行的最大并发请求数目，默认值8</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、能够被单个IP处理的并发请求数，默认值0，代表无限制，需要注意两点</span></span><br><span class="line"><span class="comment">#I、如果不为零，那CONCURRENT_REQUESTS_PER_DOMAIN将被忽略，即并发数的限制是按照每个IP来计算，而不是每个域名</span></span><br><span class="line"><span class="comment">#II、该设置也影响DOWNLOAD_DELAY，如果该值不为零，那么DOWNLOAD_DELAY下载延迟是限制每个IP而不是每个域</span></span><br><span class="line"><span class="comment">#CONCURRENT_REQUESTS_PER_IP = 16</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、如果没有开启智能限速，这个值就代表一个规定死的值，代表对同一网址延迟请求的秒数</span></span><br><span class="line"><span class="comment">#DOWNLOAD_DELAY = 3</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第三部分：智能限速/自动节流：AutoThrottle extension&lt;===</span></span><br><span class="line"><span class="comment">#一：介绍</span></span><br><span class="line"><span class="keyword">from</span> scrapy.contrib.throttle <span class="keyword">import</span> AutoThrottle <span class="comment">#http://scrapy.readthedocs.io/en/latest/topics/autothrottle.html#topics-autothrottle</span></span><br><span class="line">设置目标：</span><br><span class="line"><span class="number">1</span>、比使用默认的下载延迟对站点更好</span><br><span class="line"><span class="number">2</span>、自动调整scrapy到最佳的爬取速度，所以用户无需自己调整下载延迟到最佳状态。用户只需要定义允许最大并发的请求，剩下的事情由该扩展组件自动完成</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#二：如何实现？</span></span><br><span class="line">在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。</span><br><span class="line">注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。 不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#三：限速算法</span></span><br><span class="line">自动限速算法基于以下规则调整下载延迟</span><br><span class="line"><span class="comment">#1、spiders开始时的下载延迟是基于AUTOTHROTTLE_START_DELAY的值</span></span><br><span class="line"><span class="comment">#2、当收到一个response，对目标站点的下载延迟=收到响应的延迟时间/AUTOTHROTTLE_TARGET_CONCURRENCY</span></span><br><span class="line"><span class="comment">#3、下一次请求的下载延迟就被设置成：对目标站点下载延迟时间和过去的下载延迟时间的平均值</span></span><br><span class="line"><span class="comment">#4、没有达到200个response则不允许降低延迟</span></span><br><span class="line"><span class="comment">#5、下载延迟不能变的比DOWNLOAD_DELAY更低或者比AUTOTHROTTLE_MAX_DELAY更高</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#四：配置使用</span></span><br><span class="line"><span class="comment">#开启True，默认False</span></span><br><span class="line">AUTOTHROTTLE_ENABLED = <span class="literal">True</span></span><br><span class="line"><span class="comment">#起始的延迟</span></span><br><span class="line">AUTOTHROTTLE_START_DELAY = <span class="number">5</span></span><br><span class="line"><span class="comment">#最小延迟</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">3</span></span><br><span class="line"><span class="comment">#最大延迟</span></span><br><span class="line">AUTOTHROTTLE_MAX_DELAY = <span class="number">10</span></span><br><span class="line"><span class="comment">#每秒并发请求数的平均值，不能高于 CONCURRENT_REQUESTS_PER_DOMAIN或CONCURRENT_REQUESTS_PER_IP，调高了则吞吐量增大强奸目标站点，调低了则对目标站点更加”礼貌“</span></span><br><span class="line"><span class="comment">#每个特定的时间点，scrapy并发请求的数目都可能高于或低于该值，这是爬虫视图达到的建议值而不是硬限制</span></span><br><span class="line">AUTOTHROTTLE_TARGET_CONCURRENCY = <span class="number">16.0</span></span><br><span class="line"><span class="comment">#调试</span></span><br><span class="line">AUTOTHROTTLE_DEBUG = <span class="literal">True</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_DOMAIN = <span class="number">16</span></span><br><span class="line">CONCURRENT_REQUESTS_PER_IP = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第四部分：爬取深度与爬取方式&lt;===</span></span><br><span class="line"><span class="comment">#1、爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度</span></span><br><span class="line"><span class="comment"># DEPTH_LIMIT = 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 后进先出，深度优先</span></span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 0</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = &#x27;scrapy.squeue.PickleLifoDiskQueue&#x27;</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = &#x27;scrapy.squeue.LifoMemoryQueue&#x27;</span></span><br><span class="line"><span class="comment"># 先进先出，广度优先</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># DEPTH_PRIORITY = 1</span></span><br><span class="line"><span class="comment"># SCHEDULER_DISK_QUEUE = &#x27;scrapy.squeue.PickleFifoDiskQueue&#x27;</span></span><br><span class="line"><span class="comment"># SCHEDULER_MEMORY_QUEUE = &#x27;scrapy.squeue.FifoMemoryQueue&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#3、调度器队列</span></span><br><span class="line"><span class="comment"># SCHEDULER = &#x27;scrapy.core.scheduler.Scheduler&#x27;</span></span><br><span class="line"><span class="comment"># from scrapy.core.scheduler import Scheduler</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、访问URL去重</span></span><br><span class="line"><span class="comment"># DUPEFILTER_CLASS = &#x27;step8_king.duplication.RepeatUrl&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第五部分：中间件、Pipelines、扩展&lt;===</span></span><br><span class="line"><span class="comment">#1、Enable or disable spider middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</span></span><br><span class="line"><span class="comment">#SPIDER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#    &#x27;Amazon.middlewares.AmazonSpiderMiddleware&#x27;: 543,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#2、Enable or disable downloader middlewares</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">   <span class="comment"># &#x27;Amazon.middlewares.DownMiddleware1&#x27;: 543,</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#3、Enable or disable extensions</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</span></span><br><span class="line"><span class="comment">#EXTENSIONS = &#123;</span></span><br><span class="line"><span class="comment">#    &#x27;scrapy.extensions.telnet.TelnetConsole&#x27;: None,</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#4、Configure item pipelines</span></span><br><span class="line"><span class="comment"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="comment"># &#x27;Amazon.pipelines.CustomPipeline&#x27;: 200,</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第六部分：缓存&lt;===</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. 启用缓存</span></span><br><span class="line"><span class="string">    目的用于将已经发送的请求或相应缓存下来，以便以后使用</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware</span></span><br><span class="line"><span class="string">    from scrapy.extensions.httpcache import DummyPolicy</span></span><br><span class="line"><span class="string">    from scrapy.extensions.httpcache import FilesystemCacheStorage</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 是否启用缓存策略</span></span><br><span class="line"><span class="comment"># HTTPCACHE_ENABLED = True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.DummyPolicy&quot;</span></span><br><span class="line"><span class="comment"># 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略</span></span><br><span class="line"><span class="comment"># HTTPCACHE_POLICY = &quot;scrapy.extensions.httpcache.RFC2616Policy&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存超时时间</span></span><br><span class="line"><span class="comment"># HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存保存路径</span></span><br><span class="line"><span class="comment"># HTTPCACHE_DIR = &#x27;httpcache&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存忽略的Http状态码</span></span><br><span class="line"><span class="comment"># HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缓存存储的插件</span></span><br><span class="line"><span class="comment"># HTTPCACHE_STORAGE = &#x27;scrapy.extensions.httpcache.FilesystemCacheStorage&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第七部分：线程池&lt;===</span></span><br><span class="line">REACTOR_THREADPOOL_MAXSIZE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Default: 10</span></span><br><span class="line"><span class="comment">#scrapy基于twisted异步IO框架，downloader是多线程的，线程数是Twisted线程池的默认大小(The maximum limit for Twisted Reactor thread pool size.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#关于twisted线程池：</span></span><br><span class="line">http://twistedmatrix.com/documents/<span class="number">10.1</span><span class="number">.0</span>/core/howto/threading.html</span><br><span class="line"></span><br><span class="line"><span class="comment">#线程池实现：twisted.python.threadpool.ThreadPool</span></span><br><span class="line">twisted调整线程池大小：</span><br><span class="line"><span class="keyword">from</span> twisted.internet <span class="keyword">import</span> reactor</span><br><span class="line">reactor.suggestThreadPoolSize(<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#scrapy相关源码：</span></span><br><span class="line">D:\python3<span class="number">.6</span>\Lib\site-packages\scrapy\crawler.py</span><br><span class="line"></span><br><span class="line"><span class="comment">#补充：</span></span><br><span class="line">windows下查看进程内线程数的工具：</span><br><span class="line">    https://docs.microsoft.com/zh-cn/sysinternals/downloads/pslist</span><br><span class="line">    或</span><br><span class="line">    https://pan.baidu.com/s/1jJ0pMaM</span><br><span class="line">    </span><br><span class="line">    命令为：</span><br><span class="line">    pslist |findstr python</span><br><span class="line"></span><br><span class="line">linux下：top -p 进程<span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#===&gt;第八部分：其他默认配置参考&lt;===</span></span><br><span class="line">D:\python3<span class="number">.6</span>\Lib\site-packages\scrapy\settings\default_settings.py</span><br></pre></td></tr></table></figure><h1 id="十三-爬取亚马逊商品信息"><a href="#十三-爬取亚马逊商品信息" class="headerlink" title="十三 爬取亚马逊商品信息"></a>十三 爬取亚马逊商品信息</h1><p><a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1eSWBDh4">链接：https://pan.baidu.com/s/1eSWBDh4</a></p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="https://jerrrywang.github.io">JerryWang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="https://jerrrywang.github.io/2019/12/15/Python/%E7%88%AC%E8%99%AB/6.Scrapy%E6%A1%86%E6%9E%B6/">https://jerrrywang.github.io/2019/12/15/Python/爬虫/6.Scrapy框架/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://jerrrywang.github.io" target="_blank">JerryWang</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Scrapy%E6%A1%86%E6%9E%B6/">Scrapy框架</a></div><div class="post_share"><div class="social-share" data-image="https://jerrrywang.github.io/2019/12/09/Python/爬虫/爬虫基本原理/view.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/12/16/Python/%E7%88%AC%E8%99%AB/7.%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB/"><img class="prev-cover" src="https://jerrrywang.github.io/2019/12/09/Python/爬虫/爬虫基本原理/view.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">分布式爬虫</div></div></a></div><div class="next-post pull-right"><a href="/2019/12/14/Python/%E7%88%AC%E8%99%AB/5.%E7%88%AC%E8%99%AB%E9%AB%98%E6%80%A7%E8%83%BD%E7%9B%B8%E5%85%B3/"><img class="next-cover" src="https://jerrrywang.github.io/2019/12/09/Python/爬虫/爬虫基本原理/view.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">爬虫高性能相关</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2019/12/09/Python/%E7%88%AC%E8%99%AB/Scrapy%E6%A1%86%E6%9E%B6/" title="Scrapy框架"><img class="cover" src="https://moyand.gitee.io/2019/12/09/Python/爬虫/爬虫基本原理/view.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-12-09</div><div class="title">Scrapy框架</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div></div><div class="comment-wrap"><div><div id="lv-container" data-id="city" data-uid="MTAyMC80NTg2Ni8yMjM3Nw"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">JerryWang</div><div class="author-info__description">再牛逼的梦想，也抵不过傻逼一样的坚持！</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">155</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">90</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">31</div></a></div><a id="card-info-btn" href="https://jerrrywang.github.io"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/JerrryWang" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:wjy_0316@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://weibo.com/allenaq/" target="_blank" title="WeiBo"><i class="fab fa-zhihu"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;低级的欲望,放纵即可获得<br>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;高级的欲望,克制才能得到</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">一 介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E5%AE%89%E8%A3%85"><span class="toc-number">2.</span> <span class="toc-text">二 安装</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7"><span class="toc-number">3.</span> <span class="toc-text">三 命令行工具</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E4%BB%A5%E5%8F%8A%E7%88%AC%E8%99%AB%E5%BA%94%E7%94%A8%E7%AE%80%E4%BB%8B"><span class="toc-number">4.</span> <span class="toc-text">四 项目结构以及爬虫应用简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94-Spiders"><span class="toc-number">5.</span> <span class="toc-text">五 Spiders</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD-Selectors"><span class="toc-number">6.</span> <span class="toc-text">六 Selectors</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83-Items"><span class="toc-number">7.</span> <span class="toc-text">七 Items</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AB-Item-Pipeline"><span class="toc-number">8.</span> <span class="toc-text">八 Item Pipeline</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B9%9D-Dowloader-Middeware"><span class="toc-number">9.</span> <span class="toc-text">九 Dowloader Middeware</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81-Spider-Middleware"><span class="toc-number">10.</span> <span class="toc-text">十 Spider Middleware</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%B8%80-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%89%A9%E5%B1%95"><span class="toc-number">11.</span> <span class="toc-text">十一 自定义扩展</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%BA%8C-settings-py"><span class="toc-number">12.</span> <span class="toc-text">十二 settings.py</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%81%E4%B8%89-%E7%88%AC%E5%8F%96%E4%BA%9A%E9%A9%AC%E9%80%8A%E5%95%86%E5%93%81%E4%BF%A1%E6%81%AF"><span class="toc-number">13.</span> <span class="toc-text">十三 爬取亚马逊商品信息</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%80%9A%E8%BF%87%20SSH%20%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E8%A1%8C%E6%9C%AC%E5%9C%B0%20Shell%20%E8%84%9A%E6%9C%AC/" title="通过 SSH 链接服务器运行本地 Shell 脚本"><img src="https://jerrrywang.github.io/2018/10/11/photo/ming_7.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="通过 SSH 链接服务器运行本地 Shell 脚本"></a><div class="content"><a class="title" href="/2022/11/07/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E9%80%9A%E8%BF%87%20SSH%20%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E8%A1%8C%E6%9C%AC%E5%9C%B0%20Shell%20%E8%84%9A%E6%9C%AC/" title="通过 SSH 链接服务器运行本地 Shell 脚本">通过 SSH 链接服务器运行本地 Shell 脚本</a><time datetime="2022-11-07T04:35:30.000Z" title="发表于 2022-11-07 12:35:30">2022-11-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/Python/%E6%A8%A1%E5%9D%97/loguru%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" title="loguru日志模块的使用"><img src="https://jerrrywang.github.io/2018/10/11/photo/ming_10.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="loguru日志模块的使用"></a><div class="content"><a class="title" href="/2022/07/13/Python/%E6%A8%A1%E5%9D%97/loguru%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86/" title="loguru日志模块的使用">loguru日志模块的使用</a><time datetime="2022-07-13T01:10:19.000Z" title="发表于 2022-07-13 09:10:19">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/13/Python/%E6%A8%A1%E5%9D%97/tarfile%E6%A8%A1%E5%9D%97/" title="tarfile压缩模块的使用"><img src="https://jerrrywang.github.io/2018/10/11/photo/ming_8.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="tarfile压缩模块的使用"></a><div class="content"><a class="title" href="/2022/07/13/Python/%E6%A8%A1%E5%9D%97/tarfile%E6%A8%A1%E5%9D%97/" title="tarfile压缩模块的使用">tarfile压缩模块的使用</a><time datetime="2022-07-13T01:10:19.000Z" title="发表于 2022-07-13 09:10:19">2022-07-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/07/12/Python/%E6%A8%A1%E5%9D%97/python%E7%BB%99%E5%9B%BE%E7%89%87%E6%89%B9%E9%87%8F%E6%B7%BB%E5%8A%A0%E6%B0%B4%E5%8D%B0filestools%E6%A8%A1%E5%9D%97/" title="python给图片批量添加水印filestools模块"><img src="https://jerrrywang.github.io/2018/10/11/photo/ming_6.jpeg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="python给图片批量添加水印filestools模块"></a><div class="content"><a class="title" href="/2022/07/12/Python/%E6%A8%A1%E5%9D%97/python%E7%BB%99%E5%9B%BE%E7%89%87%E6%89%B9%E9%87%8F%E6%B7%BB%E5%8A%A0%E6%B0%B4%E5%8D%B0filestools%E6%A8%A1%E5%9D%97/" title="python给图片批量添加水印filestools模块">python给图片批量添加水印filestools模块</a><time datetime="2022-07-12T01:00:19.000Z" title="发表于 2022-07-12 09:00:19">2022-07-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/18/Python/%E6%A8%A1%E5%9D%97/python%E6%9F%A5%E6%89%BE%E6%96%87%E4%BB%B6filestools%E3%80%81glob%E6%A8%A1%E5%9D%97/" title="python查找文件fnmatch、glob模块"><img src="https://jerrrywang.github.io/2018/10/11/photo/ming_10.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="python查找文件fnmatch、glob模块"></a><div class="content"><a class="title" href="/2022/06/18/Python/%E6%A8%A1%E5%9D%97/python%E6%9F%A5%E6%89%BE%E6%96%87%E4%BB%B6filestools%E3%80%81glob%E6%A8%A1%E5%9D%97/" title="python查找文件fnmatch、glob模块">python查找文件fnmatch、glob模块</a><time datetime="2022-06-18T01:00:19.000Z" title="发表于 2022-06-18 09:00:19">2022-06-18</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By JerryWang</div><div class="footer_custom_text"><a href="https://jerrrywang.github.io">生活要像水墨画一样的浓厚多姿</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js").then(()=>{pangu.autoSpacingPage()})}function panguInit(){GLOBAL_CONFIG_SITE.isPost&&panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><div class="js-pjax"><script>function loadLivere(){var e,t,o,r;"object"==typeof LivereTower?window.LivereTower.init():(e=document,t="script",r=e.getElementsByTagName(t)[0],"function"!=typeof LivereTower&&((o=e.createElement(t)).src="https://cdn-city.livere.com/js/embed.dist.js",o.async=!0,r.parentNode.insertBefore(o,r)))}{function loadOtherComment(){loadLivere()}loadLivere()}</script></div><div class="aplayer no-destroy" data-id="60198" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="true" data-order="random"></div><script id="canvas_nest" defer color="0,0,255" opacity="0.7" zindex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="I,LOVE,YOU,JerryWang,超帥,富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="true" async></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1/dist/APlayer.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/aplayer@1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors=['meta[property="og:image"]','meta[property="og:title"]','meta[property="og:url"]',"head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/music/"]):not([href="/no-pjax/"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:send",(function(){if(window.tocScrollFn&&window.removeEventListener("scroll",window.tocScrollFn),window.scrollCollect&&window.removeEventListener("scroll",scrollCollect),"object"==typeof preloader&&preloader.initLoading(),document.getElementById("rightside").style.cssText="opacity: ''; transform: ''",window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode")})),document.addEventListener("pjax:complete",(function(){window.refreshFn(),document.querySelectorAll("script[data-pjax]").forEach(e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)}),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof chatBtnFn&&chatBtnFn(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"object"==typeof _hmt&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll(),"object"==typeof preloader&&preloader.endLoading()})),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script>var font="Ringing"</script><script data-pjax>function electric_clock_injector_config(){var c=document.getElementsByClassName("sticky_layout")[0];console.log("已挂载electric_clock"),c.insertAdjacentHTML("afterbegin",'<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img id="card-clock-loading" src="https://jerrrywang.github.io/api/shubiao/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading" class="entered loading"></div></div></div></div></div>')}document.getElementsByClassName("sticky_layout")[0]&&(location.pathname,1)&&electric_clock_injector_config()</script><script src="https://jerrrywang.github.io/api/jquery.min.js"></script><script>loc=$.ajax({url:"https://ipapi.co/json/",type:"GET",dataType:"json",async:!1,success:function(e){}}),ip=$.parseJSON(loc.responseText).ip,version=$.parseJSON(loc.responseText).version,city=$.parseJSON(loc.responseText).city,country=$.parseJSON(loc.responseText).country_name</script><script data-pjax src="https://jerrrywang.github.io/api/npm/hexo-electric-clock-plus/clock.js"></script></body></html>